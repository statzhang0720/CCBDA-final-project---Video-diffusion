{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"gpuClass":"standard","accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"MFhmbuzfXxoh"},"outputs":[],"source":["!pip install video_diffusion_pytorch\n","!pip install einops"]},{"cell_type":"code","source":["!pip install kaggle\n","%cd \"/content/\"\n","!mkdir CCBDAHW1\n","%cd /content/CCBDAHW1\n"],"metadata":{"id":"52MU_051Xyv8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bz63mDcvTLi3","outputId":"4738734e-fa98-4ee5-d1af-2f182d895eab","executionInfo":{"status":"ok","timestamp":1673335950218,"user_tz":-480,"elapsed":24422,"user":{"displayName":"張高第","userId":"00064648085028582870"}}},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["gpu_info = !nvidia-smi \n","gpu_info = '\\n'.join(gpu_info) \n","if gpu_info.find('failed') >= 0: \n","    print('Not connected to a GPU') \n","else: \n","    print(gpu_info)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8dmADv4Gctou","executionInfo":{"status":"ok","timestamp":1673350278651,"user_tz":-480,"elapsed":6,"user":{"displayName":"張高第","userId":"00064648085028582870"}},"outputId":"b4174a89-6e45-46d9-fbbc-ee5fa1c04786"},"execution_count":69,"outputs":[{"output_type":"stream","name":"stdout","text":["Tue Jan 10 11:31:17 2023       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   69C    P0    32W /  70W |  12594MiB / 15109MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","+-----------------------------------------------------------------------------+\n"]}]},{"cell_type":"code","source":["%cd /content/CCBDAHW1/\n","import os\n","os.environ['KAGGLE_CONFIG_DIR'] = \"/content/CCBDAHW1/\"\n","!kaggle competitions download -c ccbda-2022-hw1\n","!unzip /content/CCBDAHW1/ccbda-2022-hw1.zip"],"metadata":{"id":"TVGlZ7b1rxG0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Import需要的套件\n","import os\n","import numpy as np\n","import cv2\n","import torch\n","import torch.nn as nn\n","import torchvision.transforms as transforms\n","import pandas as pd\n","from torch.utils.data import DataLoader, Dataset\n","import time\n","from natsort import natsorted \n","from google.colab.patches import cv2_imshow\n","from tqdm.auto import tqdm\n","import random\n","from sklearn.model_selection import train_test_split\n","from torch.utils import data\n","import copy\n","from torch.cuda.amp import autocast, GradScaler\n","from einops import rearrange\n","from pathlib import Path\n","from torch.optim import Adam\n","from PIL import Image\n","from PIL import ImageFile\n","ImageFile.LOAD_TRUNCATED_IMAGES = True\n","from tqdm import tqdm\n","from functools import partial\n","import torch.nn.functional as F\n","from torch.utils import data\n","import os\n","from torchvision import transforms as T, utils\n","import torch\n"],"metadata":{"id":"BUjQdBk_cI_U","executionInfo":{"status":"ok","timestamp":1673336185355,"user_tz":-480,"elapsed":4826,"user":{"displayName":"張高第","userId":"00064648085028582870"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["#5 16 32 38\n","def make_dataset(data_path):\n","    samples = []\n","    for i in ['5', '16', '32', '38']:\n","        for j in os.listdir(data_path + \"/\" + i):\n","            video_path = data_path + \"/\" + i + \"/\" + j\n","            samples.append(video_path)\n","    return samples\n","\n","IMG_SIZE = 32\n","Frame_size = 8\n","def crop_center_square(frame):\n","    y, x = frame.shape[0:2]\n","    min_dim = min(y, x)\n","    start_x = (x // 2) - (min_dim // 2)\n","    start_y = (y // 2) - (min_dim // 2)\n","    return frame[start_y : start_y + min_dim, start_x : start_x + min_dim]\n"],"metadata":{"id":"2uRTraMNcJuv","executionInfo":{"status":"ok","timestamp":1673336186710,"user_tz":-480,"elapsed":2,"user":{"displayName":"張高第","userId":"00064648085028582870"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["[range(0,int(10),(int(10)//3))]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yd2kIOWVxWyt","outputId":"c2936c32-b448-45bd-bf3b-0415c523c3e6"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[range(0, 10, 3)]"]},"metadata":{},"execution_count":48}]},{"cell_type":"code","source":["class ActiontrainDataset(Dataset):\n","    def __init__(self,path,tfm):\n","      super(Dataset, self).__init__()\n","      self.path = path\n","      self.samples = make_dataset(self.path)\n","      self.transform = tfm\n","        \n","    def __len__(self):\n","      return len(self.samples)\n","\n","    def load_video_tor(self,vid_path,use_transform):\n","      cap = cv2.VideoCapture(vid_path)\n","      frames = []\n","      try:\n","          while True:\n","              ret, frame = cap.read()\n","              if not ret:\n","                pad_num = Frame_size-len(frames)\n","                if pad_num > 0:\n","                  for j in range(pad_num):\n","                    frames.append(frames[int(len(frames)-1)])\n","                if pad_num <=0:\n","                  frames = frames[int((len(frames)//2)-Frame_size/2):int((len(frames)//2)+Frame_size/2)]\n","                break\n","              frame = crop_center_square(frame)\n","              frame = cv2.resize(frame, (IMG_SIZE,IMG_SIZE))\n","              #frame = frame[:, :, [2, 1, 0]]\n","              if use_transform is not None:\n","                frame = use_transform(frame)\n","              frames.append(frame)\n","          #frames_out = frames_out[::2]\n","      finally:\n","          cap.release()\n","      return rearrange(torch.stack(frames,dim = 0), 'f c h w -> c f h w')\n","\n","    def __getitem__(self,idx):\n","      video_path = self.samples[idx]\n","      video = self.load_video_tor(video_path,self.transform)\n","      \n","      return video"],"metadata":{"id":"KKsTQwqecMVl","executionInfo":{"status":"ok","timestamp":1673336190595,"user_tz":-480,"elapsed":459,"user":{"displayName":"張高第","userId":"00064648085028582870"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["from torch.utils.data.sampler import SubsetRandomSampler\n","from sys import path\n","from torchvision.transforms import Compose, ToTensor\n","from torch.utils.data import DataLoader\n","batch_size = 32\n","\n","train_tfm = transforms.Compose([\n","    transforms.ToTensor(),\n","])\n","\n","train_dataset =ActiontrainDataset(\"/content/CCBDAHW1/train\", tfm = train_tfm)\n","train_data_loader = DataLoader(train_dataset, batch_size=batch_size, num_workers=0, shuffle=True)\n"],"metadata":{"id":"OxGhtFTIn2tg","executionInfo":{"status":"ok","timestamp":1673336194212,"user_tz":-480,"elapsed":1037,"user":{"displayName":"張高第","userId":"00064648085028582870"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["train_dataset[539].shape"],"metadata":{"id":"o5Gtnof2zrIs"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Train"],"metadata":{"id":"YkGEgPfpkDsQ"}},{"cell_type":"code","source":["def num_to_groups(num, divisor):\n","    groups = num // divisor\n","    remainder = num % divisor\n","    arr = [divisor] * groups\n","    if remainder > 0:\n","        arr.append(remainder)\n","    return arr\n","class EMA():\n","    def __init__(self, beta):\n","        super().__init__()\n","        self.beta = beta\n","\n","    def update_model_average(self, ma_model, current_model):\n","        for current_params, ma_params in zip(current_model.parameters(), ma_model.parameters()):\n","            old_weight, up_weight = ma_params.data, current_params.data\n","            ma_params.data = self.update_average(old_weight, up_weight)\n","\n","    def update_average(self, old, new):\n","        if old is None:\n","            return new\n","        return old * self.beta + (1 - self.beta) * new\n","CHANNELS_TO_MODE = {\n","    1 : 'L',\n","    3 : 'RGB',\n","    4 : 'RGBA'\n","}\n","\n","def seek_all_images(img, channels = 3):\n","    assert channels in CHANNELS_TO_MODE, f'channels {channels} invalid'\n","    mode = CHANNELS_TO_MODE[channels]\n","\n","    i = 0\n","    while True:\n","        try:\n","            img.seek(i)\n","            yield img.convert(mode)\n","        except EOFError:\n","            break\n","        i += 1\n","def gif_to_tensor(path, channels = 3, transform = T.ToTensor()):\n","    img = Image.open(path)\n","    tensors = tuple(map(transform, seek_all_images(img, channels = channels)))\n","    return torch.stack(tensors, dim = 1)\n","\n","def identity(t, *args, **kwargs):\n","    return t\n","\n","def cast_num_frames(t, *, frames):\n","    f = t.shape[1]\n","\n","    if f == frames:\n","        return t\n","\n","    if f > frames:\n","        return t[:, :frames]\n","\n","    return F.pad(t, (0, 0, 0, 0, 0, frames - f))\n","def video_tensor_to_gif(tensor, path, duration = 120, loop = 0, optimize = True):\n","    images = map(T.ToPILImage(), tensor.unbind(dim = 1))\n","    first_img, *rest_imgs = images\n","    first_img.save(path, save_all = True, append_images = rest_imgs, duration = duration, loop = loop, optimize = optimize)\n","    return images"],"metadata":{"id":"SLHDw1oHklCC","executionInfo":{"status":"ok","timestamp":1673336199944,"user_tz":-480,"elapsed":452,"user":{"displayName":"張高第","userId":"00064648085028582870"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["import torch\n","from video_diffusion_pytorch import Unet3D, GaussianDiffusion, Trainer\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","\n","model = Unet3D(\n","    dim = 64,\n","    use_bert_text_cond = False,  # this must be set to True to auto-use the bert model dimensions\n","    dim_mults = (1, 2, 4, 8),\n",").to(device)\n","\n","\n","diffusion = GaussianDiffusion(\n","    model,\n","    image_size = 32,    # height and width of frames\n","    num_frames = 8,     # number of video frames\n","    timesteps = 1000,  # number of steps\n","    text_use_bert_cls = False,  \n","    loss_type = 'l1'    # L1 or L2\n",").to(device)"],"metadata":{"id":"QbuhhcfulCsb","executionInfo":{"status":"ok","timestamp":1673336210393,"user_tz":-480,"elapsed":6102,"user":{"displayName":"張高第","userId":"00064648085028582870"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["from torch.optim import Adam\n","batch = 32\n","ema = EMA(0.995)\n","ema_model = copy.deepcopy(diffusion)\n","train_data = train_data_loader\n","optimizer = Adam(diffusion.parameters(), lr = 1e-4)\n","scaler = GradScaler(enabled = True)"],"metadata":{"id":"kiYWgDXnkDPG","executionInfo":{"status":"ok","timestamp":1673336211728,"user_tz":-480,"elapsed":8,"user":{"displayName":"張高第","userId":"00064648085028582870"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["data = torch.load('/content/drive/MyDrive/Final Project/data/MSRVTT/videos/results/model_notext-current.pt')\n","\n","step = data['step'] # epoochs 改(step +1,500)\n","diffusion.load_state_dict(data['model'])\n","ema_model.load_state_dict(data['ema'])\n","scaler.load_state_dict(data['scaler'])"],"metadata":{"id":"QD3N6QTf8JCM","executionInfo":{"status":"ok","timestamp":1673336227620,"user_tz":-480,"elapsed":15900,"user":{"displayName":"張高第","userId":"00064648085028582870"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":["epochs = 500\n","for epoch in range(step +1,1000):\n","  \n","  for train_idx in tqdm(train_data):\n","    videos = train_idx\n","    with autocast(enabled = True):\n","      loss = diffusion(\n","          videos.to(device),\n","          cond = None\n","      )\n","      scaler.scale(loss).backward()\n","  print(f\"Epoch: {epoch}, Loss: {loss.item():.3f}\")\n","  scaler.step(optimizer)\n","  scaler.update()\n","  optimizer.zero_grad()\n","\n","  if epoch % 10 == 0:\n","    if epoch < 2000:\n","      ema_model.load_state_dict(diffusion.state_dict())\n","    ema.update_model_average(ema_model, diffusion)\n","\n","  if epoch != 0 and epoch % 5 == 0:\n","    milestone = epoch // 5\n","    num_samples = 4\n","    batches = num_to_groups(num_samples, batch)\n","\n","    all_videos_list = list(map(lambda n: ema_model.sample(batch_size=n), batches))\n","    all_videos_list = torch.cat(all_videos_list, dim = 0)\n","\n","    all_videos_list = F.pad(all_videos_list, (2, 2, 2, 2))\n","\n","    one_gif = rearrange(all_videos_list, '(i j) c f h w -> c f (i h) (j w)', i = 2)\n","    video_path = f'/content/drive/MyDrive/Final Project/data/MSRVTT/videos/results_surf/{milestone}.gif'\n","    video_tensor_to_gif(one_gif, video_path)\n","\n","    data = {\n","    'step': epoch,\n","    'model': diffusion.state_dict(),\n","    'ema': ema_model.state_dict(),\n","    'scaler': scaler.state_dict()\n","    }\n","    torch.save(data, '/content/drive/MyDrive/Final Project/data/MSRVTT/videos/results/model_notext-current.pt')\n","    \n","    print(f'Succefully save model-{milestone}')"],"metadata":{"id":"ixnQ0nPJln_T"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["num_samples = 1\n","batches = num_to_groups(num_samples, batch)\n","\n","for i in range(100):\n","  all_videos_list = list(map(lambda n: ema_model.sample(batch_size=n), batches))\n","  all_videos_list = torch.cat(all_videos_list, dim = 0)\n","  one_gif = rearrange(all_videos_list, '(i j) c f h w -> c f (i h) (j w)', i = 1)\n","  video_path = f'/content/drive/MyDrive/Final Project/data/MSRVTT/videos/results_surf/output_gif/gif{i}.gif'\n","  video_tensor_to_gif(one_gif, video_path)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":729},"id":"6qJNbNTrNbqE","executionInfo":{"status":"error","timestamp":1673347926502,"user_tz":-480,"elapsed":888494,"user":{"displayName":"張高第","userId":"00064648085028582870"}},"outputId":"c9967a35-b06f-47f1-a619-35f4e5f29869"},"execution_count":68,"outputs":[{"output_type":"stream","name":"stderr","text":["sampling loop time step: 100%|██████████| 1000/1000 [00:43<00:00, 22.92it/s]\n","sampling loop time step: 100%|██████████| 1000/1000 [00:43<00:00, 23.12it/s]\n","sampling loop time step: 100%|██████████| 1000/1000 [00:43<00:00, 23.07it/s]\n","sampling loop time step: 100%|██████████| 1000/1000 [00:45<00:00, 21.79it/s]\n","sampling loop time step: 100%|██████████| 1000/1000 [00:43<00:00, 22.89it/s]\n","sampling loop time step: 100%|██████████| 1000/1000 [00:44<00:00, 22.59it/s]\n","sampling loop time step: 100%|██████████| 1000/1000 [00:46<00:00, 21.71it/s]\n","sampling loop time step: 100%|██████████| 1000/1000 [00:44<00:00, 22.24it/s]\n","sampling loop time step: 100%|██████████| 1000/1000 [00:46<00:00, 21.65it/s]\n","sampling loop time step: 100%|██████████| 1000/1000 [00:46<00:00, 21.63it/s]\n","sampling loop time step: 100%|██████████| 1000/1000 [00:48<00:00, 20.74it/s]\n","sampling loop time step: 100%|██████████| 1000/1000 [00:46<00:00, 21.62it/s]\n","sampling loop time step: 100%|██████████| 1000/1000 [00:46<00:00, 21.50it/s]\n","sampling loop time step: 100%|██████████| 1000/1000 [00:46<00:00, 21.58it/s]\n","sampling loop time step: 100%|██████████| 1000/1000 [00:44<00:00, 22.42it/s]\n","sampling loop time step: 100%|██████████| 1000/1000 [00:47<00:00, 20.87it/s]\n","sampling loop time step: 100%|██████████| 1000/1000 [00:46<00:00, 21.56it/s]\n","sampling loop time step: 100%|██████████| 1000/1000 [00:46<00:00, 21.66it/s]\n","sampling loop time step: 100%|██████████| 1000/1000 [00:46<00:00, 21.63it/s]\n","sampling loop time step:  46%|████▌     | 457/1000 [00:21<00:25, 21.45it/s]\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-68-ea288aa39c3b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m   \u001b[0mall_videos_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mema_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m   \u001b[0mall_videos_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_videos_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m   \u001b[0mone_gif\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrearrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_videos_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'(i j) c f h w -> c f (i h) (j w)'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-68-ea288aa39c3b>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(n)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m   \u001b[0mall_videos_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mema_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m   \u001b[0mall_videos_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_videos_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m   \u001b[0mone_gif\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrearrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_videos_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'(i j) c f h w -> c f (i h) (j w)'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/video_diffusion_pytorch/video_diffusion_pytorch.py\u001b[0m in \u001b[0;36msample\u001b[0;34m(self, cond, cond_scale, batch_size)\u001b[0m\n\u001b[1;32m    689\u001b[0m         \u001b[0mchannels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchannels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    690\u001b[0m         \u001b[0mnum_frames\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_frames\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 691\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mp_sample_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchannels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_frames\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcond\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcond\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcond_scale\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcond_scale\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    692\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    693\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minference_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/video_diffusion_pytorch/video_diffusion_pytorch.py\u001b[0m in \u001b[0;36mp_sample_loop\u001b[0;34m(self, shape, cond, cond_scale)\u001b[0m\n\u001b[1;32m    674\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    675\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreversed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_timesteps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'sampling loop time step'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_timesteps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 676\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mp_sample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfull\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcond\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcond\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcond_scale\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcond_scale\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    678\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0munnormalize_img\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/video_diffusion_pytorch/video_diffusion_pytorch.py\u001b[0m in \u001b[0;36mp_sample\u001b[0;34m(self, x, t, cond, cond_scale, clip_denoised)\u001b[0m\n\u001b[1;32m    660\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mp_sample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcond\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcond_scale\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1.\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclip_denoised\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    661\u001b[0m         \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 662\u001b[0;31m         \u001b[0mmodel_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_log_variance\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mp_mean_variance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclip_denoised\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclip_denoised\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcond\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcond\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcond_scale\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcond_scale\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    663\u001b[0m         \u001b[0mnoise\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    664\u001b[0m         \u001b[0;31m# no noise when t == 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/video_diffusion_pytorch/video_diffusion_pytorch.py\u001b[0m in \u001b[0;36mp_mean_variance\u001b[0;34m(self, x, t, clip_denoised, cond, cond_scale)\u001b[0m\n\u001b[1;32m    637\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    638\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mp_mean_variance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclip_denoised\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcond\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcond_scale\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1.\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 639\u001b[0;31m         \u001b[0mx_recon\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_start_from_noise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnoise\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdenoise_fn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_with_cond_scale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcond\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcond\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcond_scale\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcond_scale\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    640\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    641\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mclip_denoised\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/video_diffusion_pytorch/video_diffusion_pytorch.py\u001b[0m in \u001b[0;36mforward_with_cond_scale\u001b[0;34m(self, cond_scale, *args, **kwargs)\u001b[0m\n\u001b[1;32m    462\u001b[0m         \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    463\u001b[0m     ):\n\u001b[0;32m--> 464\u001b[0;31m         \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnull_cond_prob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    465\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcond_scale\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhas_cond\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    466\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/video_diffusion_pytorch/video_diffusion_pytorch.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, time, cond, null_cond_prob, focus_present_mask, prob_focus_present)\u001b[0m\n\u001b[1;32m    525\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    526\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 527\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinal_conv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    529\u001b[0m \u001b[0;31m# gaussian diffusion trainer class\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1188\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1191\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    202\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    205\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1188\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1191\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    611\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    612\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 613\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    614\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    615\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    606\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroups\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    607\u001b[0m             )\n\u001b[0;32m--> 608\u001b[0;31m         return F.conv3d(\n\u001b[0m\u001b[1;32m    609\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdilation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroups\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    610\u001b[0m         )\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"markdown","source":["for FID"],"metadata":{"id":"xrYu7sJunTDJ"}},{"cell_type":"code","source":["from torchvision.utils import save_image, make_grid"],"metadata":{"id":"evtYjL1mo2K5","executionInfo":{"status":"ok","timestamp":1673336679967,"user_tz":-480,"elapsed":1,"user":{"displayName":"張高第","userId":"00064648085028582870"}}},"execution_count":17,"outputs":[]},{"cell_type":"code","source":["num_samples = 1\n","batches = num_to_groups(num_samples, batch)\n","\n","for i in range(100):\n","  all_videos_list = list(map(lambda n: ema_model.sample(batch_size=n), batches))\n","  all_videos_list = torch.cat(all_videos_list, dim = 0)\n","\n","  #all_videos_list = F.pad(all_videos_list, (2, 2, 2, 2))\n","\n","  one_gif = rearrange(all_videos_list[0,], 'c f h w -> f c h w')\n","  os.mkdir(f'/content/drive/MyDrive/Final Project/data/MSRVTT/videos/results_surf/output/video{i}/')\n","  for j in range(8):\n","    video_path = f'/content/drive/MyDrive/Final Project/data/MSRVTT/videos/results_surf/output/video{i}/frame{j}.jpg'\n","    save_image(one_gif[j,:] ,video_path)"],"metadata":{"id":"aQp3UvEDKmBZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for i in range(750+790+793+759):\n","  ori_vid = rearrange(train_dataset[i], 'c f h w -> f c h w')\n","  os.mkdir(f'/content/CCBDAHW1/videos/video{i}/')\n","  for j in range(8):\n","    video_path = f'/content/CCBDAHW1/videos/video{i}/frame{j}.jpg'\n","    save_image(ori_vid[j,:] ,video_path)"],"metadata":{"id":"SXlWsZtZqrpo","executionInfo":{"status":"ok","timestamp":1673337898774,"user_tz":-480,"elapsed":90065,"user":{"displayName":"張高第","userId":"00064648085028582870"}}},"execution_count":36,"outputs":[]},{"cell_type":"code","source":["one_gif[1,:].shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Z70sP_TnoXWO","executionInfo":{"status":"ok","timestamp":1673336895344,"user_tz":-480,"elapsed":3,"user":{"displayName":"張高第","userId":"00064648085028582870"}},"outputId":"b1b002a6-3a36-4410-c666-1af74d5e538c"},"execution_count":22,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([3, 32, 32])"]},"metadata":{},"execution_count":22}]},{"cell_type":"code","source":["!find /content/CCBDAHW1/train/5/ -name '*.mp4' | wc -l\n","!find /content/CCBDAHW1/train/16/ -name '*.mp4' | wc -l\n","!find /content/CCBDAHW1/train/32/ -name '*.mp4' | wc -l\n","!find /content/CCBDAHW1/train/38/ -name '*.mp4' | wc -l"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"txr9m6fUr2Xd","executionInfo":{"status":"ok","timestamp":1673337548277,"user_tz":-480,"elapsed":904,"user":{"displayName":"張高第","userId":"00064648085028582870"}},"outputId":"62c02877-fb48-456d-9f61-0215773edcdf"},"execution_count":31,"outputs":[{"output_type":"stream","name":"stdout","text":["750\n","790\n","793\n","759\n"]}]},{"cell_type":"code","source":["import shutil"],"metadata":{"id":"XBfKhimzETYB","executionInfo":{"status":"ok","timestamp":1673343885929,"user_tz":-480,"elapsed":2,"user":{"displayName":"張高第","userId":"00064648085028582870"}}},"execution_count":53,"outputs":[]},{"cell_type":"code","source":["for i in range(100):\n","  shutil.copyfile(f\"/content/drive/MyDrive/Final Project/data/MSRVTT/videos/results_surf/output/video{i}/frame2.jpg\", f\"/content/drive/MyDrive/Final Project/data/MSRVTT/videos/results_surf/output2/img{i}-2.jpg\")\n"],"metadata":{"id":"SHpe0qCqEFC3","executionInfo":{"status":"ok","timestamp":1673344142807,"user_tz":-480,"elapsed":1089,"user":{"displayName":"張高第","userId":"00064648085028582870"}}},"execution_count":57,"outputs":[]},{"cell_type":"code","source":["for i in range(3092):\n","  shutil.copyfile(f\"/content/CCBDAHW1/videos/video{i}/frame2.jpg\", f\"/content/CCBDAHW1/image/img{i}-2.jpg\")\n"],"metadata":{"id":"fuIcYszPFhF1","executionInfo":{"status":"ok","timestamp":1673344338801,"user_tz":-480,"elapsed":460,"user":{"displayName":"張高第","userId":"00064648085028582870"}}},"execution_count":58,"outputs":[]},{"cell_type":"code","source":["!python /content/calc_metrics_for_dataset.py \\\n"," --real_data_path \"/content/CCBDAHW1/videos\" \\\n"," --fake_data_path \"/content/drive/MyDrive/Final Project/data/MSRVTT/videos/results_surf/output\" \\\n"," --mirror 1 --gpus 1 --resolution 32 --metrics fvd2048_8f --verbose 10 --use_cache 0"],"metadata":{"id":"KrPuFR_l9_kG","executionInfo":{"status":"ok","timestamp":1673346015459,"user_tz":-480,"elapsed":1652,"user":{"displayName":"張高第","userId":"00064648085028582870"}}},"execution_count":63,"outputs":[]},{"cell_type":"code","source":["import zipfile\n","from zipfile import ZipFile\n","import os\n","os.chdir('/content/drive/MyDrive/Colab Notebooks')  # 針對 Colab 改變路徑，本機環境可不用\n","\n","with zipfile.ZipFile('test.zip', mode='w') as zf:\n","    for i in range(3092):\n","      zf.write(f\"/content/CCBDAHW1/image/img{i}-2.jpg\")"],"metadata":{"id":"MwNa4a2hG9qW","executionInfo":{"status":"ok","timestamp":1673344671239,"user_tz":-480,"elapsed":1552,"user":{"displayName":"張高第","userId":"00064648085028582870"}}},"execution_count":61,"outputs":[]},{"cell_type":"code","source":["!pip install pytorch_gan_metrics\n","!python -m pytorch_gan_metrics.calc_metrics \\\n","--path \"/content/drive/MyDrive/Final Project/data/MSRVTT/videos/results_surf/output2\" \\\n","--stats \"/content/drive/MyDrive/Colab Notebooks/test.zip\""],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pLPuVseHDIqT","executionInfo":{"status":"ok","timestamp":1673344726991,"user_tz":-480,"elapsed":10063,"user":{"displayName":"張高第","userId":"00064648085028582870"}},"outputId":"415b0e4c-b42e-4ec8-f1d9-3d1ebfb2f1bb"},"execution_count":62,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: pytorch_gan_metrics in /usr/local/lib/python3.8/dist-packages (0.5.2)\n","Requirement already satisfied: torchvision>=0.9.2 in /usr/local/lib/python3.8/dist-packages (from pytorch_gan_metrics) (0.14.0+cu116)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from pytorch_gan_metrics) (21.3)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from pytorch_gan_metrics) (4.64.1)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.8/dist-packages (from pytorch_gan_metrics) (1.7.3)\n","Requirement already satisfied: torch>=1.8.2 in /usr/local/lib/python3.8/dist-packages (from pytorch_gan_metrics) (1.13.0+cu116)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch>=1.8.2->pytorch_gan_metrics) (4.4.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from torchvision>=0.9.2->pytorch_gan_metrics) (2.25.1)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.8/dist-packages (from torchvision>=0.9.2->pytorch_gan_metrics) (7.1.2)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from torchvision>=0.9.2->pytorch_gan_metrics) (1.21.6)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging->pytorch_gan_metrics) (3.0.9)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision>=0.9.2->pytorch_gan_metrics) (1.24.3)\n","Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision>=0.9.2->pytorch_gan_metrics) (4.0.0)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision>=0.9.2->pytorch_gan_metrics) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision>=0.9.2->pytorch_gan_metrics) (2022.12.7)\n","/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py:554: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n","Traceback (most recent call last):\n","  File \"/usr/lib/python3.8/runpy.py\", line 194, in _run_module_as_main\n","    return _run_code(code, main_globals, None,\n","  File \"/usr/lib/python3.8/runpy.py\", line 87, in _run_code\n","    exec(code, run_globals)\n","  File \"/usr/local/lib/python3.8/dist-packages/pytorch_gan_metrics/calc_metrics.py\", line 26, in <module>\n","    (IS, IS_std), FID = get_inception_score_and_fid(\n","  File \"/usr/local/lib/python3.8/dist-packages/pytorch_gan_metrics/utils.py\", line 90, in get_inception_score_and_fid\n","    mu, sigma = f['mu'][:], f['sigma'][:]\n","  File \"/usr/local/lib/python3.8/dist-packages/numpy/lib/npyio.py\", line 260, in __getitem__\n","    raise KeyError(\"%s is not a file in the archive\" % key)\n","KeyError: 'mu is not a file in the archive'\n"]}]}]}